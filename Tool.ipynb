{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import feature_selection as skfs\n",
    "from sklearn import preprocessing as skpp\n",
    "from sklearn import model_selection as ms\n",
    "import numpy as np\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummify(dataframe, *not_to_dummy, dummy_na=True):\n",
    "    print(\"\"\"DUMMIFYING: Creating dummy columns from categorical variables. This is necessary as sci-kit learn models always treat feature\n",
    "    columns as continuous variables. Therefore, we change categorical variables to binary pseudo-continuous variables\n",
    "    ({0})\"\"\".format(dt.datetime.now()))\n",
    "\n",
    "    print(\"\"\"Dummifying (Step1): We make a deep copy of the dataframe. If we did not do this, Python would change our \n",
    "    original dataframe, but we don't want this\"\"\")\n",
    "    dummy_tmp1 = dataframe.copy(deep=True)\n",
    "    \n",
    "    print(\"\"\"Dummifying (Step2): dummy_tmp2: We drop columns from the table that we do not want to be dummified\"\"\")\n",
    "    dummy_tmp2 = dummy_tmp1.drop(not_to_dummy, axis=1, errors='ignore')\n",
    "    \n",
    "    print(\"\"\"Dummifying (Step3): dummy_tmp3: Transforming all categorical columns of dummy_tmp2 to dummy-variables\"\"\")\n",
    "    dummy_tmp3 = pd.get_dummies(dummy_tmp2, dummy_na=dummy_na)\n",
    "\n",
    "    print(\"\"\"Dummifying (Step4): Concatenating (aka. joining)\"\"\")\n",
    "    print(\"\"\"Dummifying (Step4a): dataframe2: Make a deep copy of the dummy_tmp3 table\"\"\")\n",
    "    dataframe2 = dummy_tmp3.copy(deep=True)\n",
    "    for col_name in not_to_dummy: # Iterate through columns\n",
    "        col = dataframe[col_name]\n",
    "        dataframe2 = pd.concat([dataframe2, col], 1)\n",
    "\n",
    "    # Print which columns were transformed:\n",
    "    print('Dummified: {}'.format(dataframe.dtypes[dataframe.dtypes == object].keys()))\n",
    "\n",
    "    return dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance_threshold_select(dataframe, thresh=0.0, na_replacement=-999):\n",
    "    print('Transforming: Deleting low variance columns ({0})'.format(dt.datetime.now()))\n",
    "\n",
    "    dataframe1 = dataframe.copy(deep=True) # Make a deep copy of the dataframe\n",
    "    selector = VarianceThreshold(thresh)\n",
    "    selector.fit(dataframe1.fillna(na_replacement)) # Fill NA values as VarianceThreshold cannot deal with those\n",
    "    dataframe2 = dataframe.loc[:,selector.get_support(indices=False)] # Get new dataframe with columns deleted that have NA values\n",
    "\n",
    "    return dataframe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_csv_file(path='Data/dataset.csv', y_value=''): # Function definition: The path is standard as Data/dataset.csv but you can change it\n",
    "    ##########\n",
    "    # PRINTING information, before function begins\n",
    "    print('Your file path is: {}. If you get a file not found error, try to change your file path. Dont forget the .csv at the end of the file'.format(path))\n",
    "    if y_value:\n",
    "        print('You have specified a y-column: {}. You get: x-table, y-variable'.format(y_value))\n",
    "    else:\n",
    "        print('You have not specified a y-column. You get: table or variable'.format(y_value))    \n",
    "    \n",
    "    ######\n",
    "    # MAIN\n",
    "    # dataframe1 is a dataframe. More information on Data Structures: http://pandas.pydata.org/pandas-docs/stable/dsintro.html\n",
    "    dataframe1 = pd.read_csv('Data/dataset.csv') # With this function, we load the CSV, the argument is the path of the CSV\n",
    "\n",
    "    if y_value: # If we defined that there is a y_value in the file, then lets create our x and y variables\n",
    "        # Creating the y and the x variable\n",
    "        y = dataframe1[y_value] # We choose our y_value column as our y column.\n",
    "        x = dataframe1.drop(y_value, axis=1) # Here, we drop our y column to create an x variable with no y variable in it\n",
    "        \n",
    "        return x, y # Returns a tuple with x, y variable. You can get them using x, y =  read_csv_file(y_value='your_value')\n",
    "    \n",
    "    else:\n",
    "        return dataframe1 # Returns a dataframe if no y_value was provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_x(dataframe, threshold=0.0): # dataframe0 stands for dataframe 0\n",
    "    dataframe1 = dataframe.copy(deep=True) # We make a deep copy of the dataframe. If we did not do this, Python would change our original dataframe, but we don't want this\n",
    "\n",
    "    dataframe2 = dummify(dataframe1) # Creating dummies from categorical variables (for more info, see http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    dataframe3 = variance_threshold_select(dataframe2, thresh=threshold) # We delete variables \n",
    "    return dataframe3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_y(column):\n",
    "    le = skpp.LabelEncoder()\n",
    "    column = le.fit_transform(column)\n",
    "    return(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your file path is: Data/dataset.csv. If you get a file not found error, try to change your file path. Dont forget the .csv at the end of the file\n",
      "You have specified a y-column: Survived. You get: x-table, y-variable\n",
      "Dummifying (2017-05-03 13:01:50.016781)\n",
      "Dummified: Index(['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')\n",
      "Transforming: Deleting low variance columns (2017-05-03 13:01:50.045801)\n"
     ]
    }
   ],
   "source": [
    "# Standardized Functions\n",
    "x, y = read_csv_file(y_value='Survived')\n",
    "x = preprocess_x(x)\n",
    "y = preprocess_y(y)\n",
    "x_train, x_test, y_train, y_test = ms.train_test_split(x, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
